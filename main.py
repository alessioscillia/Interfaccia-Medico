import streamlit as st
from PIL import Image
import threading
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive
from streamlit_gsheets import GSheetsConnection
import pandas as pd
import io
import logging
from datetime import datetime
import os
import json


# Prefer zoneinfo (Python 3.9+); se non disponibile usa pytz se presente, altrimenti fallback a UTC
try:
    from zoneinfo import ZoneInfo
    _HAS_ZONEINFO = True
except Exception:
    ZoneInfo = None
    _HAS_ZONEINFO = False
    try:
        import pytz
    except Exception:
        pytz = None


def _now_rome_str():
    """Return current time in Europe/Rome as formatted string. Falls back to UTC if tz libs missing."""
    fmt = '%Y-%m-%d %H:%M:%S'
    try:
        if _HAS_ZONEINFO and ZoneInfo is not None:
            return datetime.now(ZoneInfo("Europe/Rome")).strftime(fmt)
        if 'pytz' in globals() and pytz is not None:
            return datetime.now(pytz.timezone("Europe/Rome")).strftime(fmt)
    except Exception:
        pass
    # Fallback
    return datetime.utcnow().strftime(fmt)

st.set_page_config(layout="wide")
st.markdown("<h2 style='margin-bottom:0;'>Valutazione qualit√† immagini colonscopiche</h2>", unsafe_allow_html=True)

user_id = st.text_input("üë®‚Äç‚öïÔ∏è Id utente:", key="user_id")
if not user_id:
    st.warning("Inserisci il tuo nome per proseguire.")
    st.stop()


# Autenticazione Google Drive tramite Service Account (nessuna autenticazione utente richiesta)
@st.cache_resource(show_spinner=False)
def get_drive():
    """Inizializza Google Drive usando un Service Account.
    
    Le credenziali possono essere fornite in due modi:
    1. File JSON locale: crea un file 'service-account.json' nella stessa directory
    2. Streamlit Secrets: aggiungi le credenziali in .streamlit/secrets.toml
    
    Per Streamlit Cloud, usa secrets.toml con questa struttura:
    [gcp_service_account]
    type = "service_account"
    project_id = "your-project-id"
    private_key_id = "your-private-key-id"
    private_key = "-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n"
    client_email = "your-service-account@your-project.iam.gserviceaccount.com"
    client_id = "your-client-id"
    auth_uri = "https://accounts.google.com/o/oauth2/auth"
    token_uri = "https://oauth2.googleapis.com/token"
    auth_provider_x509_cert_url = "https://www.googleapis.com/oauth2/v1/certs"
    client_x509_cert_url = "your-cert-url"
    """
    gauth = GoogleAuth()
    
    # Determina quale file di credenziali usare
    if "gcp_service_account" in st.secrets:
        # Usa Streamlit secrets (per deployment su Streamlit Cloud)
        service_account_info = dict(st.secrets["gcp_service_account"])
        
        # Salva temporaneamente le credenziali in un file
        temp_cred_file = "temp_service_account.json"
        with open(temp_cred_file, "w") as f:
            json.dump(service_account_info, f)
        
        service_account_file = temp_cred_file
        client_email = service_account_info.get('client_email')
    else:
        # Usa il file locale service-account.json
        service_account_file = "service-account.json"
        if not os.path.exists(service_account_file):
            st.error(f"""
            ‚ö†Ô∏è **File delle credenziali non trovato!**
            
            Per usare questa applicazione, devi:
            1. Creare un Service Account su Google Cloud Console
            2. Scaricare il file JSON delle credenziali
            3. Salvarlo come `{service_account_file}` nella stessa directory di questo script
            
            Oppure configura le credenziali in `.streamlit/secrets.toml` per il deployment.
            
            Consulta il file SETUP_SERVICE_ACCOUNT.md per istruzioni dettagliate.
            """)
            st.stop()
        
        # Leggi l'email del client dal file
        with open(service_account_file, 'r') as f:
            service_account_info = json.load(f)
            client_email = service_account_info.get('client_email')
    
    # Configura PyDrive2 per usare il service account
    gauth.settings['client_config_backend'] = 'service'
    gauth.settings['service_config'] = {
        'client_json_file_path': service_account_file,
        'client_user_email': client_email,  # Questo campo √® richiesto da PyDrive2
    }
    
    # Autentica usando il service account
    gauth.ServiceAuth()
    
    drive = GoogleDrive(gauth)
    return drive

drive = get_drive()


# Cache per download immagine: usa l'id del file per evitare di serializzare l'oggetto PyDrive
@st.cache_data(show_spinner=False)
def get_image_bytes_by_id(file_id: str):
    """Scarica i bytes dell'immagine da Google Drive usando l'id del file.

    Viene memorizzato in cache da Streamlit per evitare ripetuti download durante la stessa sessione.
    """
    f = drive.CreateFile({'id': file_id})
    buf = f.GetContentIOBuffer()
    return buf.read()

# ID della cartella principale 'Articolo Polyps'
ARTICOLO_POLYPS_FOLDER_ID = '1He7eQCE2xI5X8n00A-B-eKEBZjNIw9cJ'

# Linee guida
linee_guida = """
- **Luminosit√†:** l'immagine deve essere ben illuminata senza aree eccessivamente scure o sovraesposte.
- **Nitidezza:** i dettagli della mucosa devono essere ben visibili, senza sfocatura dovuta a motion blur.
- **Colori naturali:** assenza di dominanti cromatiche innaturali.
- **Assenza di artefatti:** evitare immagini disturbate da artefatti digitali o movimenti improvvisi.
- **Composizione:** la porzione di interesse deve essere centrata e visibile.
"""

# MODIFICA QUI NUMERO DI IMMAGINI PER DATASET DA MOSTRARE
IMAGES_PER_DATASET = 3

# ‚úÖ OTTIMIZZAZIONE 1: Cache del listing cartelle/immagini (evita chiamate API ad ogni rerun)
@st.cache_data(show_spinner="Caricamento immagini da Google Drive...", ttl=3600)
def load_all_images_from_drive():
    """Carica tutte le cartelle e immagini da Google Drive. Cachato per 1 ora."""
    # Recupera tutte le sottocartelle ('Dataset 1', 'Dataset 2', 'Dataset 3')
    folder_list = drive.ListFile(
        {'q': f"'{ARTICOLO_POLYPS_FOLDER_ID}' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false"}
    ).GetList()
    
    all_images_by_dataset = {}
    for folder in folder_list:
        images = drive.ListFile(
            {'q': f"'{folder['id']}' in parents and trashed=false and mimeType contains 'image/'"}
        ).GetList()
        # Salviamo solo i metadati essenziali (non l'intero oggetto PyDrive)
        all_images_by_dataset[folder['title']] = [
            {
                "img_obj": {'id': img['id'], 'title': img['title']},
                "folder_name": folder['title']
            }
            for img in images
        ]
    return all_images_by_dataset

all_images_by_dataset = load_all_images_from_drive()

if not all_images_by_dataset or all(len(v) == 0 for v in all_images_by_dataset.values()):
    st.warning("Nessuna immagine trovata nelle sottocartelle.")
    st.stop()


# Funzione per ottenere le immagini assegnate all'utente attuale
def get_user_images(user_id: str):
    """Determina il gruppo di assegnazione dell'utente e restituisce le immagini da valutare.
    
    Ogni 3 utenti consecutivi riceve lo stesso set di immagini.
    Es: utenti 1,2,3 -> set A; utenti 4,5,6 -> set B; utenti 7,8,9 -> set C; utenti 10,11,12 -> set A (ricomincia)
    """
    # Leggi gli utenti unici gi√† presenti su Google Sheets
    logger = logging.getLogger(__name__)
    try:
        conn = st.connection("gsheets", type=GSheetsConnection)
        dati = conn.read(worksheet="Foglio1").fillna("")
        if not dati.empty and "id_utente" in dati.columns:
            unique_users = dati["id_utente"].unique().tolist()
        else:
            unique_users = []
    except Exception as e:
        # Log the error for diagnosis and show a lightweight warning to the user.
        logger.exception("Errore lettura Google Sheets")
        st.warning("Impossibile leggere gli utenti da Google Sheets; verr√† usata una lista vuota (fallback).")
        unique_users = []
    
    # Se l'utente √® gi√† in lista, la sua posizione √® quella
    if user_id in unique_users:
        user_position = unique_users.index(user_id)
    else:
        # Se non √® in lista, sar√† il prossimo utente dopo gli ultimi
        user_position = len(unique_users)
    
    # Determina il gruppo di assegnazione (ogni 3 utenti)
    group_number = user_position // 3
    
    # Raccogli tutte le immagini organizzate per dataset
    total_datasets = len(all_images_by_dataset)
    images_per_group = IMAGES_PER_DATASET * total_datasets
    total_possible_images = sum(len(imgs) for imgs in all_images_by_dataset.values())

    # Protezione: se per qualche motivo non ci sono immagini totali, evita divisione/modulo per zero
    if total_possible_images == 0:
        logging.getLogger(__name__).warning("Nessuna immagine disponibile in all_images_by_dataset")
        return []
    
    # Calcola l'indice di inizio per questo gruppo
    group_start_idx = (group_number * images_per_group) % total_possible_images
    
    # Costruisci la lista di immagini per questo utente (IMAGES_PER_DATASET per ogni dataset)
    user_images = []
    sorted_datasets = sorted(all_images_by_dataset.keys())
    
    for dataset_idx, dataset_name in enumerate(sorted_datasets):
        dataset_images = all_images_by_dataset[dataset_name]
        if len(dataset_images) > 0:
            # Calcola l'offset per questo dataset e gruppo
            dataset_offset = (group_start_idx + (dataset_idx * IMAGES_PER_DATASET)) % len(dataset_images)
            # Seleziona fino a IMAGES_PER_DATASET immagini da questo dataset (se il dataset √® pi√π piccolo,
            # prendiamo solo quanto √® disponibile per evitare duplicati nell'elenco risultante)
            take_count = min(IMAGES_PER_DATASET, len(dataset_images))
            selected = [
                dataset_images[(dataset_offset + i) % len(dataset_images)]
                for i in range(take_count)
            ]
            user_images.extend(selected)
    
    return user_images


# Selezione basata su gruppo di utenti
if "immagini" not in st.session_state:
    st.session_state.immagini = get_user_images(user_id)
    st.session_state.indice = 0
    st.session_state.valutazioni = []

indice = st.session_state.indice
imgs = st.session_state.immagini

if indice < len(imgs):
    curr_entry = imgs[indice]
    img_file = curr_entry["img_obj"]
    folder_name = curr_entry["folder_name"]

    # Usa la funzione cache per scaricare i bytes (solo la prima volta per id)
    file_id = img_file['id']
    try:
        img_bytes = get_image_bytes_by_id(file_id)
        image = Image.open(io.BytesIO(img_bytes))
    except Exception as e:
        st.error(f"Errore nel download dell'immagine: {e}")
        image = None

    col1, col2 = st.columns([2, 1])
    with col1:
        if image is not None:
            st.image(image, width='stretch')
        st.markdown(f"<b>Dataset:</b> {folder_name}", unsafe_allow_html=True)  # Mostra il nome del dataset
        score = st.slider("Score di qualit√† (1 = pessima, 10 = ottima)", 1, 10, 5, key=f"score_{indice}")
        
        col_btn1, col_btn2 = st.columns(2)
        with col_btn1:
            if st.button("‚¨ÖÔ∏è Indietro"):
                if indice > 0:
                    # Rimuovi l'ultima valutazione per poter ricominciare da quella precedente
                    if st.session_state.valutazioni:
                        st.session_state.valutazioni.pop()
                    st.session_state.indice -= 1
                    st.rerun()
        
        with col_btn2:
            if st.button("Salva voto per questa immagine ‚ûú"):
                st.session_state.valutazioni.append({
                    "id_utente": user_id,
                    "nome_immagine": img_file['title'],
                    "score": score,
                    "dataset": folder_name,    # aggiunge anche il nome del dataset al record
                    "timestamp": _now_rome_str()
                })
                st.session_state.indice += 1
                st.rerun()  # Necessario per passare subito all'immagine successiva
    
    with col2:
        st.markdown("### Linee guida qualit√†")
        st.markdown(linee_guida)
    
    st.markdown(f"<center><small>{indice} / {len(imgs)} immagini valutate</small></center>", unsafe_allow_html=True)
    # Prefetch della prossima immagine in background (non blocca la UI)
    next_idx = indice + 1
    if next_idx < len(imgs):
        try:
            next_id = imgs[next_idx]['img_obj']['id']
            # l'invocazione a get_image_bytes_by_id salver√† in cache i bytes
            threading.Thread(target=get_image_bytes_by_id, args=(next_id,), daemon=True).start()
        except Exception:
            pass
else:
    st.success("Hai completato tutte le valutazioni!")
    df = pd.DataFrame(st.session_state.valutazioni)
    st.dataframe(df)
    
    # ‚úÖ OTTIMIZZAZIONE 3: Salvataggio con retry e gestione errori graceful
    if "salvato" not in st.session_state:
        st.session_state.salvato = False
    
    if not st.session_state.salvato:
        with st.spinner("Salvataggio risultati su Google Sheets..."):
            try:
                conn = st.connection("gsheets", type=GSheetsConnection)
                dati = conn.read(worksheet="Foglio1").fillna("")
                df_tot = pd.concat([dati, df], ignore_index=True)
                conn.update(worksheet="Foglio1", data=df_tot)
                st.session_state.salvato = True
                st.success("‚úÖ Risultati salvati con successo!")
            except Exception as e:
                st.error(f"‚ö†Ô∏è Errore durante il salvataggio: {e}")
                st.info("Puoi scaricare i risultati localmente usando il bottone qui sotto.")
                # Offri download CSV come fallback
                csv = df.to_csv(index=False)
                st.download_button(
                    label="üì• Scarica risultati (CSV)",
                    data=csv,
                    file_name=f"valutazioni_{user_id}_{_now_rome_str().replace(' ', '_').replace(':', '-')}.csv",
                    mime="text/csv"
                )
    else:
        st.success("‚úÖ Risultati gi√† salvati!")